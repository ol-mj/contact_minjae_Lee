
# [ 로지스틱 회귀 ]

이 글은 공돌이의 수학정리 노트 포스팅을 보고 공부한 내용을 바탕으로 이해해 가는 과정을 적은 글입니다.

처음에 이해가 되지 않았던 것 

1. **Sigmoid 함수를 쓰는 이유는 독립변수 x들의 각 클래스에 대한 분포가 정규분포를 따를 것으로 가정하기 때문이다. (무슨 말인지 모르곘음)**
2. Sigmoid함수의 형태를 결정하기 위해서 parameter a, b를 결정해야 한다. (Sigmoid 함수의 형태가 결과에 어떤 영향을 미치며, 이게 어떤 의미를 가지는 지 잘 모르겠다)

헷갈린 것

1. 합성함수 편미분, 미분
2. P의 편미분을 보며 벡터화 시킨 것

---

# [ 로지스틱 회귀의 목적 ]

선형회귀는 데이터들을 어떠한 직선 함수로 근사 시키는 것이 목표라면

로지스틱 회귀는 어떠한 데이터들을 S자 형태의 함수로 근사 시키는 것이 목표다.

로지스틱 회귀는 범주형 데이터, 즉 남자, 여자 / Yes or No / 강아지 고양이 같은 라벨이 범주형이며 연속적인 숫자로 나타내기 어려운 데이터들을 이야기 한다.

보통은 0 or 1로 숫자로 치환하여 생각한다.

---

# [ 로지스틱 회귀에서는 어떤 함수를 사용할까? ]

$$
S(x)=\frac{1}{1+exp(-x)} ~~~~~(1)\\ S(x):Sigmoid~Function
$$

Sigmoid Function : 범주형 데이터에 대해 회귀분석을 할 때 사용한다.

범주형 데이터에 대한 모델을 세우기 위해서는 필요한 함수는 어떤 값을 넘어가기 전에는 0, 넘어간 뒤에는 1을 가지는 형태의 함수여야 하기 때문

Yes or No의 개념이다.

S자 커브 함수에 대한 여러가지 후보가 있지만

**Sigmoid 함수를 쓰는 이유는 독립변수 x들의 각 클래스(0또는 1)에 대한 분포가 정규분포를 따를 것으로 가정하기 때문이다. (무슨 말인지 모르곘음)**

Sigmoid 결정하기 위해 파라미터 a, b를 sigmoid 함수의 식에 넣어준다.

ax+b를 특정 함수에 넣을 꼴이 된다.

$$
S(x)=\frac {1} {1+exp(-(ax+b))} ~~~~~ (2)
$$

a는 가파른 정도(0에서 1으로 올라가는 속도가), b는 sigmoid 함수를 좌우로 움직여주는 역할을 한다. (*공돌이의 수학정리노트님의 포스팅을 통해 시각적으로 보면 더욱 편하다.)

우리가 정해야 하는 값은 a와 b의 값이다. Error를 최소화 하는 방식으로 a와 b를 정할 수 있다.

즉 시그모이드 함수가 데이터를 나타내는 함수

선형회귀에서는 데이터를 나타내는 함수가 직선

(a는 왜 정해야하고 b는 좀 이해가 되긴함 a는 가파를 수록 좋은거 아닌가?)

---

# [ Error 함수 정의 ]

우리는 이 범주형 데이터를 근사 시킬 함수를 정했으니, 이 Sigmoid 함수를 데이터를 가장 잘 근사하게 만들기 위하여 Error를 정의하고 Error를 최소화 시킬 것이다.

$$
P=\frac {1} {1+exp(-(ax+b))} ~~~~~ (3) \\ 0 <P<1
$$

[ E**rror Function에 대하여 생각을 해보고 넘어가자 ]**

무엇을 변수로 사용할 것인가?

Error란 데이터와 근사시킨 함수의 오차이다.

그러므로 Error Function은 P와 y를 변수로 이루어진 함수이다.

그렇다면 Error Fuction은 어떻게 정의해야 좋을까?

맞추면 작은 값을가지고 맞추지 못하면 최대한 큰 값을 가지는 함수를 찾는다.

미분하기 편한 함수

정답을 맞추었다는 기준을 [ y=1, P=0.5 이상] [ y=0, P=0.5 이하] 라고 가정한다면

근사 함수가 정답을 맞추었을 때는 E가 최대한 0에 가깝게

= Sigmoid 함수가 데이터를 근사 했을 때

근사 함수가 정답을 맞추지 못 했을 때는 E가 최대한 1에 가깝게

= Sigmoid 함수가 데이터를 근사 시키지 못 했을 때

그리고 로그 함수를 이용하여 그런 함수를 나타낼 수 있다.

$$
E(y,P)=\begin{cases} -log(P)~~~~~if~~y=1 \\ -log(1-P)~~~~~if~~y=0\end {cases}~~~~~(4)
$$

밑에 함수를 확인 할 수 있는 코드를 첨부한다.

함수의 꼴을 확인해보면

y=1 일 때 P가 0.5이상 (맞춤)이라면 Error Function의 값은 매우 작아지고

P가 0.5이하 (틀림) 라면 Error Function의 값이 매우 커진다.

또한 위 (4)의 식을

$$
E(y,P)=-y*log(P)+(1-y)*log(1-P)
$$

의 꼴로 나타낼 수 있다.

- 공부하면서 생각흐름
    
    
    여기서 y가 무엇인가?
    
    주어진 데이터이다 (x,y) ex) 남자 y=0, 여자 y=1
    
    그리고 이 데이터를 근사하는 함수를 구하는 것
    
    시그모이드 함수로 얻게 되는 값을 P라고 이름 붙인다
    
    P라고 붙이는 이유는 라벨에 대한 확률 값을 출력해준다고 생각할 수 있다.
    
    P가 0.5보다 크거나 같으면 데이터에 대한 라벨을 1로 예측
    
    그렇지 못하면 라벨을 0으로 예측한다.
    

---

# [ 편미분과 벡터화 ]

Error Function을 정의했으니 이제 경사하강법을 통하여 Error Function을 최소화 시키는 단계로 넘아간다. 하지만 그 전에 포스팅에서 나온 개념이 헷갈려 다시 짚고 넘어간다

포스팅을 보며 이해가 되지 않았던 수식

편의를 위해 벡터화 하여 편미분을 하는 것이었다.

$$
\theta=\begin{bmatrix} a\\ \\ b\end{bmatrix} \\ \frac {\partial{E}} {\partial{\theta}}=  \begin{bmatrix} \frac {\partial{E}} {\partial{a}}   \\ \\ \frac {\partial{E}} {\partial{b}}\end{bmatrix}
$$

라고 생각하면 된다.

---

# [ Error에 대한 gradient 계산하기 ]

## ( P에 대한 편미분 계산 )

P에 대한 편미분을 먼저 계산하는 이유는 Error Function의 편미분 과정에서 P에 대한 편미분 꼴이 튀어나오기 때문에 미리 계산해준다.

$$
X=\begin{bmatrix} x \\ 1\end{bmatrix} \\ \theta = \begin{bmatrix} a \\ b\end{bmatrix} \\~\\ P(X,\theta)= \frac{1}{1+exp(-\theta^TX)} \\~\\ \frac{\partial{P}}{\partial{\theta}} = \frac{\partial} {\partial{\theta}} \left( \frac{1}{1+exp(-\theta^TX)} \right) \\~\\ \frac{\partial{P}}{\partial{\theta}}=P(X,\theta)(1-P(X,\theta))X \\~\\ \begin{bmatrix} \frac {\partial{P}} {\partial{a}}   \\~\\ \frac {\partial{P}} {\partial{b}}\end{bmatrix} = P(X,\theta)(1-P(X,\theta)) \begin{bmatrix} x \\ \\ 1\end{bmatrix} \\
$$

자세한 계산 과정은 생략한다.

직관적이지 않은 벡터 편미분만을 표기 하는 목적으로 작성하였다.

또한 P(X,theta)는 벡터가 들어있다고 당황하기 보다는

P(X,theta)라는 함수는 X와 theta라는 벡터의 인풋을 받아

하나의 출력 값을 갖는 함수로 생각하면 된다. (위의 식 (2),(3)과 동일)

(함수는 정의역의 각 원소에 대하여 공역의 원소가 오직 하나 대응되는 집합이므로 **오직 하나의 원소가 대응되는** **집합이라고 해서 원소가 꼭 하나의 성분일 필요는 없다)**

**또한 벡터 편미분에서 주의할 점은 무엇이 벡터 or 행렬이고 무엇이 스칼라 값인지 인지하는 것이다.**

## ( Error에 대한 편미분 계산 )

$$
E(y,P)=-y*log(P)+(1-y)*log(1-P) \\ \frac {\partial{E}} {\partial{\theta}} = (P-y)X \\ ~\\ \begin{bmatrix} \frac {\partial{E}} {\partial{a}}   \\~\\ \frac {\partial{E}} {\partial{b}}\end{bmatrix} = (P-y) \begin{bmatrix} x \\ \\ 1\end{bmatrix} \\
$$

---

# [ 파이썬 코드 ]



- 의문점
    
    
    그런데 너무 P의 작은 차이에서도 하나의 Error가 
    
    여러개 Error의 가중치를 가질 수 있는 것 아닌가?
    
    log함수니까 P의 작은 차이에도… x의 차이가 작으면 괜찮은가
    
    지금은 생각할 거 아닌듯
    

- 참고자료