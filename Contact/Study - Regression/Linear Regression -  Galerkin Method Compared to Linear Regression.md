# Galerkin Method 의미 (feat.선형회귀)

---

# [ 글의 목적 ]

이미 알고 있는 사전지식과 비슷한 패턴으로 어떤 개념을 이해하는 것은 사전지식에 대한 이해가 깊을 수록 효율적이거나 효과적이라고 생각한다. 그리고 나는 이런 방법을 좋아하는데 사전지식에 대한 이해가 깊은 쪽은 아니고, 그저 이런 패턴은 공통점과 차이점이 흥미롭기 때문이다. 
Galerkin Method와 Linear Regression의 비교를 통한 두 가지 방법에 대한 직관적인 이해

Galerkin Method를 배우며 수식적으로 이해가 갔지만 명쾌하게 와닿지 않던 부분을 공돌이의 수학정리 노트님의 선형회귀 포스팅를 보며 Galerkin Method와 비슷해 보여 비교를 해보니 매우 구조가 똑같았고

검색을 해보니 사실 둘은 같은 방법이라는 것을 알게되었다.

Galerkin은 연속적인 함수를 다루고

선형회귀는 이산적인 벡터를 다루기에 선형회귀는 조금 더 직관적이었다.

==여기서 얻을 수 있는 현재 내게 가장 큰 효용은 선형회귀를 대수 방정식꼴의 형태보다 좀 더 벡터의 관점 및 형태로서 이해함으로서 이것을 더 잘 다룰 수 있는 이해 중 하나가 아닐까 기대한다.==

두 방법을 알아보고 비교해보며 직관적으로 이해한 과정을 담은 글이다.

---

# [ Galerkin Method란? ]

갤러킨 근사는 미분방정식의 해를 기저함수(basis function)의 선형 결합으로 표현하는 근사를 일컬으며, 이 근사를 이용하여 미분방정식의 해를 구하는 방법을 갤러킨 방법이라 한다.

가중치는 오차(즉, 해석해와 근사해의 차)를 전 영역에 대하여 적분하였을 때 최소가 되도록 하는 것으로 정하며, 오차가 선택된 각각의 모든 기저함수들과 직교성(orthgonality)을 가진다는 원리가 핵심이다. 여기서 갤러킨 방법은 가중 잔차법(weighted residual method)의 일종으로서 가중 함수(weight function)를 기저함수와 동일하게 사용하는 방법으로 이해할 수 있다.

> 출처 : [https://m.terms.naver.com/entry.naver?docId=5826923&cid=64656&categoryId=64656](https://m.terms.naver.com/entry.naver?docId=5826923&cid=64656&categoryId=64656)
> 

---

# [ Galerkin method 예시를 통한 설명 ]

예시를 통하여 Galerkin Method가 어떻게 해를 근사시키는 지 본다.

$$
\frac {d^2u} {dx^2} +p(x)=0,~~~~0\leq x \leq 1  \\boundary ~condition\\u(0)=0\\ \frac {du} {dx} (1)=1
$$

위 미분 방정식은

**u(x) : exact solution (엄밀해, 실제해)** 가 존재한다 (미분 방정식에 따라 다름)

하지만 미분방정식을 풀어 엄밀해를 구하기 어려운 경우 근사적인 해를 가정하고 푼다.

 **u’(x) : approximate solution (근사해)**

$$
\frac {d^2u'} {dx^2}+p(x)=R(x) \\R(x)~:~error=\frac{d^2u'} {dx^2}-\frac{d^2u} {dx^2}
$$

근사시킨 해이기 때문에 오차(R(x))가 존재한다

그리고 오차가 가장 작은 근사해를 구하는 것이 Galerkin Method의 목표다.

### [ 가중잔여법 ]

$$
\int _{0}^{1} R(x)W(x)dx = 0
$$

**W(x)를 가중함수라고 한다.**

그리고 위의 **오차의 가중평균**이 0이 되게 하는 방법을 **가중잔여법이라고** 한다.

위의 **오차의 가정평균이** 0이 될 때 오차가 가장 작은 근사해가 된다.

그리고 그 예시가 유한요소법에서의  **‘갤러킨 방법’**이다.

그리고 그 가중함수를 유한요소법 에서는 **시험함수(trial function)로** 부르며

$$
 u'(x) =\sum_{i=1}^{N} c_i\phi_i(x)\\c_i:coefficient \\\phi_i(x) : trial~function
$$

갤러킨에서는 미분방정식의 근사해를 시험함수의 일차결합으로 나타낸다.

$$
\int_{0}^{1}R(x)\phi_i(x)dx=0,~~i=1,...,N
$$

따라서 이런 가중잔여법을 다음과 같이 나타낼 수 있다.

그리고 풀이는 이 글에서는 다루지 않을 것이다.

---

# [ Galerkin Method가 의미하는 바 ]

처음 유한요소해석에서 Galerkin Method의 강의를 들었을 때

도대체 왜 저렇게 하는 것이 좋은 근사해를 만들 수 있는지 도무지 감이 잡히지 않았다.

이해가 잘 되지 않았을 때 3가지정도의 의문이 머릿속에서 맴돌았다 

1. 왜  저게 가장 좋은 근사해지?
2. 왜 시험함수와 적분해줄까?
3. 오차가 0이 된다?라는 의미인가

그리고 이 의문은 선형대수학에서 함수를 벡터로 보는 시각에서 명쾌해졌다.

이 글을 이해하기 위해선 아래의 개념에 대한 이해가 필요하다.

- 함수는 무한한 차원을 가진 벡터공간에서의 하나의 벡터이다.
- 그리고 함수는 기저함수들의 선형결합으로 나타낼 수 있다.
- 면에 수직한 직선은 그 면의 모든 선분과 직교한다.
- 행렬을 기저의 변환의 관점에서 보는 시각


1. **trial function**을 기저로 하는 벡터 공간 V를 만든다 

  **(trial function = basis,  V = span(basis.1, basis.2, … basis.n)**

1. 그 벡터공간내의 어떤 하나의 벡터는 엄밀해와 가장 가까운 벡터가 될 것이다
2. 가장 가까운 벡터는 엄밀해가 벡터공간에 정사영 시킨 곳에 존재한다
3. 엄밀해와 엄밀해와 가장 가까운의 벡터의 차이는 **R(x)**가 된다
4. **R(x)**는 **trial function**들과 모두 수직이다 (벡터공간에 수직인 벡터이므로)
5. 서로 수직인 벡터는 내적하면 0이 된다. 함수의 내적은 적분으로 나타낼 수 있다.

라고 생각을 하면 매우 명쾌하다

### Galerkin Method는 우리가 정한 기저함수를 정하여 어떤 미분방정식의 해를 근사시키려 했을 때 계수(좌표)중 가장 엄밀해와 가까운 해를 구할 수 있게 해준다

하지만 함수를 벡터로 생각하여 생각하는 건 우리는 따로 훈련을 하지 않으면 3차원 이상의 공간을 상상하는 것이 어렵다.

그래서 더 직관적인 이해는 가중잔여법 중 하나인 최소제곱법을 통해 하는 것이 매우  편하다

원리는 같으나 우리가 익숙한 행렬과 벡터이기 때문에 더 그렇다

**그리고 그 것을 가장 시각화를 잘 해준 자료가 공돌이의 수학정리 노트님의**

**선형회귀에 관한 글이다**

---

# [ Linear Regression의 선형대수학 관점의 해석 ]

[선형회귀 - 공돌이의 수학정리노트](https://angeloyeo.github.io/2020/08/24/linear_regression.html)

포스팅의 내용을 간략히 요약하자면

**선형회귀는** 간단히 표현하자면 데이터들을 가장 잘 근사하는 직선을 구하는 것이다.

$$
[~f(x)=mx+b~ ]~\\~\\Data:(-1,0),~(0,1), ~(0,3) \\f(-1)=-m+b=0 \\ f(0)=0+b=1 \\f(0)=0+b=3\\A\vec{x}=\vec{b}~~~-\triangleright~~ \begin{bmatrix} -1 & 1 \\ 0 & 1 \\ 0 & 1 \end {bmatrix} \begin{bmatrix} m \\ b \end{bmatrix}= \begin{bmatrix} 0\\1\\3\end {bmatrix} \\ \vec{a_1}= First~ column ~vector~ of ~A
\\ \vec{a_2} =Second~column~vector~of~A
\\ 
$$

우리는 직선을 구하기 위해서는 **직선의 기울기,  y절편** 을 구해야한다

그러므로 위의 수식에서 가장 데이터를 잘 근사하는 즉 **벡터x (**m,b)를 찾는 것이 우리의 목적이고

선형회귀에서 가장 데이터를 잘 근사한다는 의미는 **모든 데이터와의 오차의 평균이 가장 낮다** 를 의미한다.

**벡터b**는 **벡터 a1, a2**의 결합으로 나타낼 수 없다. 즉 한 직선으로 저 데이터들을 모두 지나갈 수 없다.

그러므로 **벡터 a1, a2**으로 만든 벡터의 선형결합을 통해 얻을 수 있는 최적의 벡터는 **벡터b**가 열벡터(a1,a2)의 span인 A의 열공간 col(A)에 정사영된 **벡터p** 라고 할 수 있다.

$$
\vec{p}:The~optimal~approximate~ solution\\
~(Linear~combination~of ~\vec{a_1},~\vec{a_2})\\~ \vec{e}:\vec{b}-\vec{p}~~(error ~vector) \\ 
$$

그리고 **벡터e** 는 열공간에 수직이므로 **벡터 a1, a2** 모두와 수직이고 벡터의 수직은 내적이 0임을 의미한다.

$$
A\cdot \vec{e}=0~~\\A^Te=0
$$

그리고 위 수식을 통하여 우리는 **벡터x**를 구할 수 있다

**(모든 데이터와의 오차가 가장 적은 기울기와 절편)**

---

# Galerkin Method와 Linear Regression

두개의 예시로 알 수 있는 것은 두 가지 모두 같은 방식으로 최적해를 구하고 있다.

당연히 둘 다 같은 가중잔차법을 이용하여 같은 개념이다

각 예시들에서 같은 것을 의미하는 것을 대조 해볼 수 있다.

$$
|~~~~\vec{e}=R(x) ~~~~|~~~~ \vec{b}=u(x) ~~~~|~~~~\vec{p} =u'(x) ~~~~|~~~~\vec{a_i}=\phi_i(x)~~~~|
$$

혹시 Galerkin Method가 이해가 안 된다면 선형회귀를 선형대수학적으로 먼저 이해하고 Galerkin Method를 배우게 된다면 훨씬 더 직관적으로 다가올 것 같다.

위의 공돌이의 수학정리 노트님의 포스팅에서 **그림5번을** 통하여

각 벡터들끼리의 관계도를 본다면 매우 명확하게 다가온다.

---

# 벡터의 거리가 가장 가까울 때, 왜 최적 해가 되었나?

마지막으로 선형대수학적으로 보게 되어 단순하지만 바로 감이 오지 않을 수도 있는 개념을 설명하고 마치겠다.

우선 위의 방법들에서 최적 해의 정의는 **모든 데이터들과의 오차의 합이 가장 작을 때이다**.

오히려 익숙한 최적화 문제 관점에서의 수식과 비교하면 간단하다

선형회귀의 직관적인 벡터의 예시로 보는 것이 훨씬 편하다.

오차의 크기는 0보다 크고 작고가 중요하지 않고 그 크기, 절대값이 중요하기 때문에

오차에서 제곱을 한다.

i개의 데이터가 주어진 상황을 가정한다면 (xi,yi)

$$
Error~from~an~Optimization~Perspective\\y'=mx+b\\Data:(x_i,~y_i) \\e_i=(y'_i-y_i)^2~~~ \\E=\frac{1}{N} \sum_{i=1}^{N}e_i =\frac{1}{N} \sum_{i=1}^{N}(y'_i-y_i)^2~=\frac{1}{N}\sum_{i=1}^{N}(mx_i+b_i-y)\\e_i:Error~of ~i.th  ~Data~\\E:The~sum~of~all~errors \\N:Number~of~Data \\
$$

우리는 이런 E 함수가 가장 작은 a,b를 찾아 최적화 시킨다.

그리고 벡터의 관점에서 본다면 완전히 같은 것을 알 수 있다.

위의 선형회귀에서 알 수 있듯이

**벡터b의 각 i번째 원소는 yi**

**벡터p의 각 i번째 원소는 y’i**

$$
\vec{e} =\vec{b}-\vec{p}=\begin{bmatrix} y_1-y'_1 \\ y_2-y'_2 \\ \vdots \\y_i-y'i\end{bmatrix} \\ e.L2~norm=\sqrt{\sum_{i=1}^{N}(y'_i-y_i)^2}
$$

그리고 벡터의 크기는 다음과 같다.

함수 E는 벡터 e의 유클리드 norm의 크기가 가장 작을 때 가장 작다.

하지만 N은 데이터의 개수이기 때문에 동일한 데이터의 개수일 때는 변수가 아닌 상수이다.

또한 **두 가지 모두 의미가 모든 오차의 크기의 합이거나 비례한다는 것을 확인 할 수 있다.**

 

- 참고자료
    
    [[유한요소법] 6. 갤러킨 방법(1), Galerkin Method](https://blog.naver.com/mykepzzang/221113579060)